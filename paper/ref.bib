@inproceedings{automl_zero,
  title        = {Automl-zero: Evolving machine learning algorithms from scratch},
  author       = {Real, Esteban and Liang, Chen and So, David and Le, Quoc},
  booktitle    = {International Conference on Machine Learning},
  pages        = {8007--8019},
  year         = {2020},
  organization = {PMLR}
}

@article{mgg,
  title   = {A new generation alternation model of genetic algorithms and its assessment},
  author  = {Sato, Hiroshi},
  journal = {Transactions of the Japanese Society for Artificial Intelligence},
  volume  = {12},
  number  = {2},
  pages   = {82--91},
  year    = {1997}
}

@article{Fahlman_1989,
  title   = {The cascade-correlation learning architecture},
  author  = {Fahlman, Scott and Lebiere, Christian},
  journal = {Advances in neural information processing systems},
  volume  = {2},
  year    = {1989},
  pages   = {524--532}
}

@inproceedings{Hutter_2011,
  title        = {Sequential model-based optimization for general algorithm configuration},
  author       = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle    = {Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5},
  pages        = {507--523},
  year         = {2011},
  organization = {Springer}
}

@inproceedings{Finn_2017,
  title        = {Model-agnostic meta-learning for fast adaptation of deep networks},
  author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle    = {International conference on machine learning},
  pages        = {1126--1135},
  year         = {2017},
  organization = {PMLR}
}

@article{Zoph_2016,
  title     = {Neural Architecture Search with Reinforcement Learning},
  author    = {Barret Zoph and Quoc Le},
  booktitle = {International Conference on Learning Representations},
  year      = {2017},
  url       = {https://openreview.net/forum?id=r1Ue8Hcxg}
}

@inproceedings{Real_2019,
  title     = {Regularized evolution for image classifier architecture search},
  author    = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle = {Proceedings of the aaai conference on artificial intelligence},
  volume    = {33},
  number    = {01},
  pages     = {4780--4789},
  year      = {2019}
}

@inproceedings{Tan_2019,
  title     = {Mnasnet: Platform-aware neural architecture search for mobile},
  author    = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {2820--2828},
  year      = {2019}
}


@article{Andrychowicz_2016,
  title   = {Learning to learn by gradient descent by gradient descent},
  author  = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal = {Advances in neural information processing systems},
  volume  = {29},
  pages   = {3981--3989},
  year    = {2016}
}

@inproceedings{Cubuk_2019,
  author    = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  title     = {AutoAugment: Learning Augmentation Strategies From Data},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  month     = {June},
  year      = {2019}
}

@inproceedings{Alet_2020,
  title     = {Meta-learning curiosity algorithms},
  author    = {Ferran Alet and Martin F. Schneider and Tomas Lozano-Perez and Leslie Pack Kaelbling},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=BygdyxHFDS}
}


@article{Elsken_2019,
  title     = {Neural architecture search: A survey},
  author    = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal   = {The Journal of Machine Learning Research},
  volume    = {20},
  number    = {1},
  pages     = {1997--2017},
  year      = {2019},
  publisher = {JMLR. org}
}

@inproceedings{Yang_2019,
  title     = {{NAS}-Bench-Suite: {NAS} Evaluation is (Now) Surprisingly Easy},
  author    = {Yash Mehta and Colin White and Arber Zela and Arjun Krishnakumar and Guri Zabergja and Shakiba Moradian and Mahmoud Safari and Kaicheng Yu and Frank Hutter},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=0DLwqQLmqV}
}

@inproceedings{Zoph_2018,
  title     = {Learning transferable architectures for scalable image recognition},
  author    = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {8697--8710},
  year      = {2018}
}

@inproceedings{So_2019,
  title        = {The evolved transformer},
  author       = {So, David and Le, Quoc and Liang, Chen},
  booktitle    = {International conference on machine learning},
  pages        = {5877--5886},
  year         = {2019},
  organization = {PMLR}
}

@article{Negrinho_2019,
  title   = {Towards modular and programmable architecture search},
  author  = {Negrinho, Renato and Gormley, Matthew and Gordon, Geoffrey J and Patil, Darshan and Le, Nghia and Ferreira, Daniel},
  journal = {Advances in neural information processing systems},
  pages   = {524--532},
  volume  = {32},
  year    = {2019}
}

@inproceedings{multi_objective_automl_zero,
  author    = {Guha, Ritam and Ao, Wei and Kelly, Stephen and Boddeti, Vishnu and Goodman, Erik and Banzhaf, Wolfgang and Deb, Kalyanmoy},
  title     = {MOAZ: A Multi-Objective AutoML-Zero Framework},
  year      = {2023},
  isbn      = {9798400701191},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3583131.3590391},
  doi       = {10.1145/3583131.3590391},
  abstract  = {Automated machine learning (AutoML) greatly eases human efforts in architecture engineering. However, mainstream AutoML methods like neural architecture search (NAS) are customized for well-designed search spaces wherein promising architectures are densely distributed. In contrast, AutoML-Zero builds machine-learning algorithms using basic primitives and can explore novel architectures beyond human knowledge. AutoML-Zero shows the potential to deploy machine learning systems by not taking advantage of either feature engineering or architectural engineering. In its current form, it only optimizes a single objective like accuracy and has no mechanism to ensure that the constraints of real-world applications are satisfied. We propose a multi-objective variant of AutoML-Zero called MOAZ, that distributes solutions on a Pareto front by trading off accuracy against the computational complexity of the machine learning algorithm. In addition to generating different Pareto-optimal solutions, MOAZ can effectively explore the sparse search space to improve search efficiency. Experimental results on linear regression tasks show MOAZ reduces the median complexity by 87.4\% compared to AutoML-Zero while accelerating the median target performance achievement speed by 82\%. In addition, our preliminary results on non-linear regression tasks show the potential for further improvements in search accuracy and for reducing the need for human intervention in AutoML.},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages     = {485–492},
  numpages  = {8},
  keywords  = {AutoML, evolutionary algorithms, multi-objective search},
  location  = {Lisbon, Portugal},
  series    = {GECCO '23}
}

@inproceedings{auto_loss_zero,
  author    = {Li, Hao and Fu, Tianwen and Dai, Jifeng and Li, Hongsheng and Huang, Gao and Zhu, Xizhou},
  title     = {AutoLoss-Zero: Searching Loss Functions From Scratch for Generic Tasks},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {1009-1018}
}

@article{vag,
  author     = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
  title      = {The program dependence graph and its use in optimization},
  year       = {1987},
  issue_date = {July 1987},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {9},
  number     = {3},
  issn       = {0164-0925},
  url        = {https://doi.org/10.1145/24039.24041},
  doi        = {10.1145/24039.24041},
  abstract   = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
  journal    = {ACM Trans. Program. Lang. Syst.},
  month      = jul,
  pages      = {319–349},
  numpages   = {31}
}